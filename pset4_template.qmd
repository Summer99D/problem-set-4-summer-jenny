---
title: "problem set IV"
authors: "Jenny Zhong & Summer Negahdar"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

"This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: **SN** **JZ**

5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps4.qmd` to an PDF file to make `ps4.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps4.qmd` and `ps4.pdf` to your github repo.
9. (Partner 1): submit `ps4.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope
 

## Download and explore the Provider of Services (POS) file

1. The variables that I pulled are: 
PRVDR_CTGRY_SBTYP_CD: subtype 
Identifies the subtype of the provider, within the primary category. Used in reporting to show the breakdown of provider categories, mainly for hospitals and SNFs.

PRVDR_CTGRY_CD: provider category 
Identifies the type of provider participating in the Medicare/Medicaid program.

PGM_TRMNTN_CD: termination code 
Indicates the providerâ€™s current termination status.

TRMNTN_EXPRTN_DT: Termination or Expiration Date
Date the provider was terminated. For CLIA providers, date the laboratory's certificate was terminated or the expiration date of the current CLIA certificate.

FAC_NAME: Facility / hospital name 
Name of the provider certified to participate in the Medicare and/or Medicaid programs.

ZIP_CD: Zip code 
Five-digit ZIP code for a provider's physical address.

CHOW_CNT: CHOW Count 
Number of times this provider has changed ownership.

CHOW_DT: CHOW Date
Effective date of the most recent change of ownership for this provider.

CITY_NAME: City
City in which the provider is physically located.

PRVDR_NUM: CMS number
Provider number 
Six or ten position identification number that is assigned to a certified provider. This is the CMS Certification Number.

2. 
```{python}
import pandas as pd
import geopandas as gpd
import shapely
import os
import matplotlib.pyplot as plt
os.chdir("/Users/samarnegahdar/Desktop/untitled folder/problem-set-4-summer-jenny")
pos2016 = pd.read_csv("pos2016.csv")
print(pos2016.head(10))
```

```{python}
#converting to string
pos2016['PRVDR_CTGRY_SBTYP_CD'] = pos2016['PRVDR_CTGRY_SBTYP_CD'].astype(str).str.zfill(2)

pos2016['PRVDR_CTGRY_CD'] = pos2016['PRVDR_CTGRY_CD'].astype(str).str.zfill(2)
pos2016.head()
```

```{python}
st_hospitals2016 = pos2016[(pos2016['PRVDR_CTGRY_SBTYP_CD'] == '1.0') & (pos2016['PRVDR_CTGRY_CD'] == '01')]

print(st_hospitals2016.head())
st_hospitals2016.shape
```

a. 7,245 hospitals are reported in this data. 
b. [To be filled in]

adding the year 2016
```{python}
st_hospitals2016.loc[:, 'YEAR'] = 2016
print(st_hospitals2016)
```

3. 
for 2017: 
```{python}
#importing pos2017
pos2017 = pd.read_csv("pos2017.csv", encoding='latin1')
```

```{python}
#converting to string
pos2017['PRVDR_CTGRY_SBTYP_CD'] = pos2017['PRVDR_CTGRY_SBTYP_CD'].astype(str).str.zfill(2)

pos2017['PRVDR_CTGRY_CD'] = pos2017['PRVDR_CTGRY_CD'].astype(str).str.zfill(2) ##filling numbers with a zero before integer
pos2017.head()
```

```{python}
#then focusing on short term hospitals 
st_hospitals2017 = pos2017[(pos2017['PRVDR_CTGRY_SBTYP_CD'] == '1.0') & (pos2017['PRVDR_CTGRY_CD'] == '01')]

print(st_hospitals2017.head())
print(st_hospitals2017.tail())
st_hospitals2017.shape
```

Adding the 2017 column 
```{python}
st_hospitals2017.loc[:, 'YEAR'] = 2017

print(st_hospitals2017)
```


for 2018: 
```{python}
#importing pos2018
pos2018 = pd.read_csv('pos2018.csv', encoding='latin1')
```

```{python}
#converting to string
pos2018['PRVDR_CTGRY_SBTYP_CD'] = pos2018['PRVDR_CTGRY_SBTYP_CD'].astype(str).str.zfill(2)

pos2018['PRVDR_CTGRY_CD'] = pos2018['PRVDR_CTGRY_CD'].astype(str).str.zfill(2)
pos2018.head()
```

```{python}
#then focus on st hospitals 
st_hospitals2018 = pos2018[(pos2018['PRVDR_CTGRY_SBTYP_CD'] == '1.0') & (pos2018['PRVDR_CTGRY_CD'] == '01')]

print(st_hospitals2018.head())
print(st_hospitals2018.tail())
st_hospitals2018.shape
```

Adding the 2018 column 
```{python}
#column had to be extended
st_hospitals2018.loc[:, 'YEAR'] = 2018

print(st_hospitals2018)
```


for 2019: 
```{python}
#importing pos2019
pos2019 = pd.read_csv('pos2019.csv', encoding='latin1')
```

```{python}
#converting to string
pos2019['PRVDR_CTGRY_SBTYP_CD'] = pos2019['PRVDR_CTGRY_SBTYP_CD'].astype(str).str.zfill(2)

pos2019['PRVDR_CTGRY_CD'] = pos2019['PRVDR_CTGRY_CD'].astype(str).str.zfill(2)
pos2019.head()
```

```{python}
#then focus on st hospitals 
st_hospitals2019 = pos2019[(pos2019['PRVDR_CTGRY_SBTYP_CD'] == '1.0') & (pos2019['PRVDR_CTGRY_CD'] == '01')]

print(st_hospitals2019.head())
print(st_hospitals2019.tail())
st_hospitals2019.shape
```

Adding the 2019 column 
```{python}
st_hospitals2019.loc[:, 'YEAR'] = 2019

print(st_hospitals2019.head(5))
```

**Appending them together**
```{python}
all_years_data = pd.concat([st_hospitals2016, st_hospitals2017, st_hospitals2018, st_hospitals2019], ignore_index=True)

print(all_years_data.head())
```

4. 
    a.
   
    b. 

## Identify hospital closures in POS file (15 pts) (*)

```{python}
unique_termination_codes = all_years_data['PGM_TRMNTN_CD'].unique()
print(unique_termination_codes)
```

1. 

```{python}
# Ensure PGM_TRMNTN_CD is treated as a string
all_years_data['PGM_TRMNTN_CD'] = all_years_data['PGM_TRMNTN_CD'].astype(str)

# Fill missing values in the PGM_TRMNTN_CD column with '1'
all_years_data['PGM_TRMNTN_CD'] = all_years_data['PGM_TRMNTN_CD'].fillna('1')

# Verify that missing values have been filled
print("Check for missing values in PGM_TRMNTN_CD column after filling:")
print(all_years_data['PGM_TRMNTN_CD'].isnull().sum())  # Should output 0 if no missing values remain

# List to store suspected closures
suspected_closures = []

# Filter for hospitals that were active in 2016
active_2016 = all_years_data[(all_years_data['YEAR'] == 2016) & (all_years_data['PGM_TRMNTN_CD'] == '0')]

# Loop through each hospital active in 2016
for _, hospital in active_2016.iterrows():
    facility_name = hospital['FAC_NAME']
    zip_code = hospital['ZIP_CD']
    
    # Check if the hospital is closed in 2019 with any termination code from ['01', '02', '03', '04']
    closed_2019 = all_years_data[
        (all_years_data['FAC_NAME'] == facility_name) &
        (all_years_data['ZIP_CD'] == zip_code) &
        (all_years_data['YEAR'] == 2019) &
        (all_years_data['PGM_TRMNTN_CD'].isin(['1', '2', '3', '4']))
    ]
    
    # Retrieve termination codes for 2018 and 2017
    termination_code_2018 = all_years_data[
        (all_years_data['FAC_NAME'] == facility_name) &
        (all_years_data['ZIP_CD'] == zip_code) &
        (all_years_data['YEAR'] == 2018)
    ]['PGM_TRMNTN_CD'].iloc[0] if not all_years_data[
        (all_years_data['FAC_NAME'] == facility_name) &
        (all_years_data['ZIP_CD'] == zip_code) &
        (all_years_data['YEAR'] == 2018)
    ].empty else None

    termination_code_2017 = all_years_data[
        (all_years_data['FAC_NAME'] == facility_name) &
        (all_years_data['ZIP_CD'] == zip_code) &
        (all_years_data['YEAR'] == 2017)
    ]['PGM_TRMNTN_CD'].iloc[0] if not all_years_data[
        (all_years_data['FAC_NAME'] == facility_name) &
        (all_years_data['ZIP_CD'] == zip_code) &
        (all_years_data['YEAR'] == 2017)
    ].empty else None
    
    # If the hospital is closed in 2019, add it to the suspected closures with 2018 and 2017 termination codes
    if not closed_2019.empty:
        suspected_closures.append({
            'FAC_NAME': facility_name,
            'ZIP_CD': zip_code,
            'year_of_closure': 2019,
            'PGM_TRMNTN_CD_2018': termination_code_2018,
            'PGM_TRMNTN_CD_2017': termination_code_2017
        })

# Convert to DataFrame and remove duplicates to ensure unique hospitals
suspected_closures_df = pd.DataFrame(suspected_closures).drop_duplicates(subset=['FAC_NAME', 'ZIP_CD'])

# Display the count of unique suspected closures
num_unique_suspected_closures = len(suspected_closures_df)
print(f"Number of unique hospitals suspected to have closed by 2019: {num_unique_suspected_closures}")
```


2. 

```{python}
print(suspected_closures_df.head(10))
```

3. 
    a.

```{python}
# Lists to store final classifications
properly_closed_hospitals = []
merge_hospitals = []

# Iterate over each hospital in suspected_closures_df
for _, hospital in suspected_closures_df.iterrows():
    facility_name = hospital['FAC_NAME']
    zip_code = hospital['ZIP_CD']
    termination_code_2018 = hospital['PGM_TRMNTN_CD_2018']
    termination_code_2017 = hospital['PGM_TRMNTN_CD_2017']
    
    # Step 1: Check the 2018 termination code
    if termination_code_2018 != '0':
        # If 2018 termination code is not 0, classify as properly closed
        properly_closed_hospitals.append({
            'FAC_NAME': facility_name,
            'ZIP_CD': zip_code,
            'PGM_TRMNTN_CD_2018': termination_code_2018,
            'PGM_TRMNTN_CD_2017': termination_code_2017
        })
    else:
        # Step 2: Check the 2017 termination code if 2018 is 0.0
        if termination_code_2017 == '0':
            # If 2017 is also 0, classify as properly closed
            properly_closed_hospitals.append({
                'FAC_NAME': facility_name,
                'ZIP_CD': zip_code,
                'PGM_TRMNTN_CD_2018': termination_code_2018,
                'PGM_TRMNTN_CD_2017': termination_code_2017
            })
        else:
            # Otherwise, classify as a potential merger
            merge_hospitals.append({
                'FAC_NAME': facility_name,
                'ZIP_CD': zip_code,
                'PGM_TRMNTN_CD_2018': termination_code_2018,
                'PGM_TRMNTN_CD_2017': termination_code_2017
            })

# Convert results to DataFrames for inspection
properly_closed_hospitals_df = pd.DataFrame(properly_closed_hospitals)
merge_hospitals_df = pd.DataFrame(merge_hospitals)

# Display results
print("Properly closed hospitals:")
print(properly_closed_hospitals_df)

print("\nHospitals suspected of merger:")
print(merge_hospitals_df)

# Number of properly closed hospitals
num_properly_closed = properly_closed_hospitals_df.shape[0]
print(f"Number of properly closed hospitals: {num_properly_closed}")

# Number of suspected mergers
num_suspected_mergers = merge_hospitals_df.shape[0]
print(f"Number of suspected mergers: {num_suspected_mergers}")
```


```{python}
# Initialize lists to store properly closed hospitals and suspected mergers
properly_closed_hospitals = []
merge_hospitals = []

# Iterate over each hospital in suspected_closures_df
for _, hospital in suspected_closures_df.iterrows():
    facility_name = hospital['FAC_NAME']
    zip_code = hospital['ZIP_CD']
    
    # Filter for this hospital's data in all_years_data for the years 2017 and 2018
    hospital_data = all_years_data[
        (all_years_data['FAC_NAME'] == facility_name) & 
        (all_years_data['ZIP_CD'] == zip_code) &
        (all_years_data['YEAR'].isin([2017, 2018]))
    ]

    # Get the termination codes for 2018 and 2017, defaulting to "1" if the year is missing
    termination_2018 = hospital_data[hospital_data['YEAR'] == 2018]['PGM_TRMNTN_CD'].iloc[0] if not hospital_data[hospital_data['YEAR'] == 2018].empty else '1'
    termination_2017 = hospital_data[hospital_data['YEAR'] == 2017]['PGM_TRMNTN_CD'].iloc[0] if not hospital_data[hospital_data['YEAR'] == 2017].empty else '1'

    # Classification logic as per the description
    if termination_2018 != '0':
        # Properly closed if terminated in 2018 (code other than "0")
        properly_closed_hospitals.append({
            'FAC_NAME': facility_name,
            'ZIP_CD': zip_code,
            'PGM_TRMNTN_CD_2017': termination_2017,
            'PGM_TRMNTN_CD_2018': termination_2018
        })
    elif termination_2018 == '0':
        # If active in 2018, check 2017 status
        if termination_2017 == '0':
            # Properly closed if also active in 2017
            properly_closed_hospitals.append({
                'FAC_NAME': facility_name,
                'ZIP_CD': zip_code,
                'PGM_TRMNTN_CD_2017': termination_2017,
                'PGM_TRMNTN_CD_2018': termination_2018
            })
        else:
            # Otherwise, add to suspected mergers
            merge_hospitals.append({
                'FAC_NAME': facility_name,
                'ZIP_CD': zip_code,
                'PGM_TRMNTN_CD_2017': termination_2017,
                'PGM_TRMNTN_CD_2018': termination_2018
            })

# Convert results to DataFrames for inspection
properly_closed_hospitals_df = pd.DataFrame(properly_closed_hospitals)
merge_hospitals_df = pd.DataFrame(merge_hospitals)
# Number of properly closed hospitals
num_properly_closed = properly_closed_hospitals_df.shape[0]
print(f"Number of properly closed hospitals: {num_properly_closed}")

# Number of suspected mergers
num_suspected_mergers = merge_hospitals_df.shape[0]
print(f"Number of suspected mergers: {num_suspected_mergers}")
```

There are no potential merged hospitals!


    a. there are no merged hospitals!
    b.

 ```{python}
    # Sort the properly closed hospitals by FAC_NAME and display the first 10 rows
sorted_properly_closed_hospitals = properly_closed_hospitals_df.sort_values(by='FAC_NAME').head(10)

# Display the sorted list
print("First 10 hospital closures sorted by name:")
print(sorted_properly_closed_hospitals)
```

## Download Census zip code shapefile (10 pt) 

1. 

```{python}
import geopandas as gpd
import pandas as pd
import matplotlib.pyplot as plt

# Load shapefile and display available columns
shapefile_path = "/Users/samarnegahdar/Desktop/untitled folder/problem-set-4-summer-jenny/gz_2010_us_860_00_500k.shp"

try:
    zip_shapes = gpd.read_file(shapefile_path)
    print("Shapefile loaded successfully.")
    print("Columns in shapefile:", zip_shapes.columns)
except Exception as e:
    print("Error loading shapefile:", e)

# Ensure column names and check if 'ZCTA5CE10' exists or an equivalent column
if 'ZCTA5CE10' in zip_shapes.columns:
    zip_column = 'ZCTA5CE10'
elif 'ZCTA5' in zip_shapes.columns:
    zip_column = 'ZCTA5'
else:
    print("ZIP code column not found. Available columns:", zip_shapes.columns)

# Step 2: Filter for Texas ZIP codes (starting with 75, 76, 77, 78, or 79)
texas_zip_shapes = zip_shapes[zip_shapes[zip_column].astype(str).str.startswith(('75', '76', '77', '78', '79'))]
print(texas_zip_shapes.head(10))

```

2. 

```{python}
#pos2016 
pos2016['ZIP_CD'] = pos2016['ZIP_CD'].astype(str).str.split('.').str[0]
pos2016['ZIP_CD'] = pos2016['ZIP_CD'].str.zfill(5)
texas_hospitals_2016 = pos2016[pos2016['ZIP_CD'].str.startswith(('75', '76', '77', '78', '79')) | (pos2016['ZIP_CD'] == '733')]

print(texas_hospitals_2016.head())
print("Number of Texas hospital records in 2016:", texas_hospitals_2016.shape[0])
```

```{python}
#counting the number of hospitals per ZIP code:
hospitals_per_zip = texas_hospitals_2016.groupby('ZIP_CD').size().reset_index(name='hospital_count')
```

```{python}
#merging for chloropleth mapping
texas_zip_shapes.loc[:, 'ZCTA5'] = texas_zip_shapes['ZCTA5'].astype(str)
hospitals_per_zip['ZIP_CD'] = hospitals_per_zip['ZIP_CD'].astype(str)

#FIXING ZIP TYPES
# Convert ZIP_CD to integer, then to string with zero-padding to ensure 5-digit format
hospitals_per_zip['ZIP_CD'] = hospitals_per_zip['ZIP_CD'].astype(float).astype(int).astype(str).str.zfill(5)

# Re-check the formatting
print("Formatted ZIP codes in hospitals_per_zip:", hospitals_per_zip['ZIP_CD'].unique()[:10])

# Now, re-run the merge
texas_zip_shapes = texas_zip_shapes.merge(hospitals_per_zip, left_on='ZCTA5', right_on='ZIP_CD', how='left')

# Fill NaN values in 'hospital_count' with 0 for ZIP codes with no hospitals
texas_zip_shapes['hospital_count'] = texas_zip_shapes['hospital_count'].fillna(0)

# Verify the number of common ZIP codes and non-zero hospital counts
print("Number of ZIP codes with hospital_count as 0:", (texas_zip_shapes['hospital_count'] == 0).sum())
print("Total ZIP codes in Texas:", texas_zip_shapes.shape[0])
print(texas_zip_shapes[['ZCTA5', 'hospital_count']].head(10))

```

```{python}
# Adjust the merge by specifying suffixes to handle duplicate columns
texas_zip_shapes = texas_zip_shapes.merge(
    hospitals_per_zip, 
    left_on='ZCTA5', 
    right_on='ZIP_CD', 
    how='left', 
    suffixes=('', '_y')  # Prevents '_x' and '_y' conflicts
)

# Drop any unnecessary duplicate columns that may arise
texas_zip_shapes = texas_zip_shapes.drop(columns=['ZIP_CD_y'], errors='ignore')

# Fill NaN values in 'hospital_count' with 0 for ZIP codes with no hospitals
texas_zip_shapes['hospital_count'] = texas_zip_shapes['hospital_count'].fillna(0)

# Verify the result
print("Number of ZIP codes with hospital_count as 0:", (texas_zip_shapes['hospital_count'] == 0).sum())
print("Total ZIP codes in Texas:", texas_zip_shapes.shape[0])
print(texas_zip_shapes[['ZCTA5', 'hospital_count']].head(10))


```

```{python}

import matplotlib.pyplot as plt

fig, ax = plt.subplots(1, 1, figsize=(12, 10))
texas_zip_shapes.plot(column='hospital_count', cmap='OrRd', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True)
ax.set_title("Number of Hospitals per ZIP Code in Texas (2016)")
ax.set_axis_off()
plt.show()

```

## Calculate zip codeâ€™s distance to the nearest hospital (20 pts) (*)

1. 

```{python}
#I will start by creating centeroids:
zip_shapes['centroids']= zip_shapes.geometry.centroid
#this will change the default geometry to centroids
all_zip_centroid= zip_shapes.copy().set_geometry('centroids')
print(all_zip_centroid.shape)
print(all_zip_centroid.columns)
print(all_zip_centroid.head(10))
```

GEO_ID: it is ta code to identify the grographic type: state, county, zipcode

ZCTA5:it is 5 digit zip code tabulation area( zpproximation of postal codes)

NAME: is the same as ZCTA5 only more user friendly. 

LSDA: legal/statistical area description categorizing geo area type( state, county,zipcode(in this case our description is zip code ZCTA5))

CENSUSAREA:The land area of the ZCTA in square miles as calculated by the Census Bureau. It measures the physical size of each area, excluding water bodies.


2. 
```{python}
from shapely.ops import unary_union

# Step 1: Filter for Texas ZIP codes (starting with 75, 76, 77, 78, or 79)
zips_texas_centroids = all_zip_centroid[all_zip_centroid['ZCTA5'].str.startswith(('75', '76', '77', '78', '79'))]

# Step 2: Filter for Texas and bordering statesâ€™ ZIP codes
zips_texas_borderstates_centroids = all_zip_centroid[all_zip_centroid['ZCTA5'].str.startswith(
    ('75', '76', '77', '78', '79', '70', '71', '72', '73', '74', '80', '81', '88', '87', '86')
)]

# Step 3: Count unique ZIP codes in each subset
num_texas_zips = zips_texas_centroids['ZCTA5'].nunique()
num_bordering_zips = zips_texas_borderstates_centroids['ZCTA5'].nunique()

print(f"Number of unique Texas ZIP codes: {num_texas_zips}")
print(f"Number of unique ZIP codes in Texas and bordering states: {num_bordering_zips}")

# Step 4: Create a function to check intersection with Texas polygon
def intersects_texas(texas_polygon, other_polygon):
    return texas_polygon.intersects(other_polygon)

# Step 5: Combine all Texas ZIP codes into a single polygon
texas_polygon = unary_union(zips_texas_centroids.geometry)

# Step 6: Apply the intersection function to find ZIP codes that are in bordering states
zips_texas_borderstates_centroids['borders_texas'] = zips_texas_borderstates_centroids.geometry.apply(
    lambda geom: intersects_texas(texas_polygon, geom)
)

# Count unique ZIP codes in subsets
unique_texas_zips = zips_texas_centroids['ZCTA5'].nunique()
unique_bordering_zips = zips_texas_borderstates_centroids[zips_texas_borderstates_centroids['borders_texas']]['ZCTA5'].nunique()

print(f"Unique Texas ZIP codes: {unique_texas_zips}")
print(f"Unique ZIP codes in Texas and bordering states: {unique_bordering_zips}")
```

3. 

```{python}
## the ZIP_CD column in our properly_closed_df is integer, I have to convert it!
# Ensure ZIP_CD in hospitals_per_zip is converted to strings without decimals
texas_hospitals_2016 = pos2016[pos2016['ZIP_CD'].str.startswith(('75', '76', '77', '78', '79')) | (pos2016['ZIP_CD'] == '733')]
hospitals_per_zip = texas_hospitals_2016.groupby('ZIP_CD').size().reset_index(name='hospital_count')
hospitals_per_zip['ZIP_CD'] = hospitals_per_zip['ZIP_CD'].astype(str).str.zfill(5)

# Also ensure ZIP codes in zips_texas_borderstates_centroids are strings with zero-padding
zips_texas_borderstates_centroids[zip_column] = zips_texas_borderstates_centroids[zip_column].astype(str).str.zfill(5)

# Verify the cleaned ZIP_CD column
print("Sample ZIP codes in hospitals_per_zip (should be string with zero-padding):")
print(hospitals_per_zip['ZIP_CD'].head())

print("Sample ZIP codes in zips_texas_borderstates_centroids (should be string with zero-padding):")
print(zips_texas_borderstates_centroids[zip_column].head())

# Proceed with merging and filtering
zips_withhospital_centroids = zips_texas_borderstates_centroids.merge(
    hospitals_per_zip,
    left_on='ZCTA5',
    right_on='ZIP_CD',
    how='inner'
)

# Filter for ZIP codes with at least one hospital
zips_withhospital_centroids = zips_withhospital_centroids[zips_withhospital_centroids['hospital_count'] > 0]

# Display the final result and count unique ZIP codes
print("zips_withhospital_centroids with at least 1 hospital:")
print(zips_withhospital_centroids.head())
num_unique_zip_codes = zips_withhospital_centroids[zip_column].nunique()
print(f"Number of unique ZIP codes with at least 1 hospital in 2016: {num_unique_zip_codes}")
```

I inner merged on zip code but had osme problems because they did not match (zip code in geo file had decimals and had to be converted)
4. 


    a.

```{python}
import numpy as np
import time
start_time_T= time.time()
# Conversion factors
LAT_TO_MILES = 69.0  # Approximate miles per degree latitude
LON_TO_MILES = 55.0  # Approximate miles per degree longitude in Texas

# Function to calculate approximate distance in miles between two points given in degrees
def degree_to_miles_distance(point1, point2):
    lat_diff = (point1.y - point2.y) * LAT_TO_MILES
    lon_diff = (point1.x - point2.x) * LON_TO_MILES
    return np.sqrt(lat_diff**2 + lon_diff**2)

# I am subsetting the df with the first 10 
Q4a_subset = zips_texas_centroids.head(10)
# Step 2: Time the nearest distance calculation
start_time = time.time()

# Calculating the nearest distances for the subset
Q4a_subset['nearest_hospital_distance'] = Q4a_subset['centroids'].apply(
    lambda x: calculate_nearest_distance(x, zips_withhospital_centroids['centroids'])
)

end_time = time.time()

# Step 3: Calculate time taken and estimate total time
subset_duration = end_time - start_time
total_zips_count = len(zips_texas_centroids)
estimated_total_duration = (subset_duration / 10) * total_zips_count

# Display the results
print(f"Time taken for 10 ZIP codes: {subset_duration:.2f} seconds")
print(f"Estimated time for entire dataset: {estimated_total_duration / 60:.2f} minutes")
```

It will take approximatelty 18 seconds to run the whole dataset!
    b.

look at part C where I calculated the actual distance. I have mentioned how much it will take to run the whole thing there. it is 36 seconds!


    c.
the unit is degree, which is approximately 69 miles or 111 km. here is convertion:

```{python}
start_time_T= time.time()

# Apply this function to each Texas ZIP code centroid and find the nearest hospital distance
zips_texas_centroids['nearest_hospital_distance_miles'] = zips_texas_centroids['centroids'].apply(
    lambda centroid: min(
        degree_to_miles_distance(centroid, hospital_centroid) 
        for hospital_centroid in zips_withhospital_centroids['centroids']
    )
)

# Calculate the average distance to the nearest hospital in miles
average_distance_miles = zips_texas_centroids['nearest_hospital_distance_miles'].mean()
end_time_T=time.time()
total_time= end_time_T - start_time_T
print(f"The average distance to the nearest hospital in Texas (in miles) is: {average_distance_miles:.2f}", "miles")
print("the total time required to calculate the prompt above is", total_time, "seconds")
```

5. 

```{python}

# Step 1: Calculate the average distance to the nearest hospital in miles
average_distance = zips_texas_centroids['nearest_hospital_distance_miles'].mean()
print(f"Average distance to the nearest hospital for each ZIP code in Texas: {average_distance:.2f} miles")

# Step 2: Map the nearest hospital distance for each ZIP code in Texas
fig, ax = plt.subplots(1, 1, figsize=(12, 10))
zips_texas_centroids.plot(column='nearest_hospital_distance_miles', cmap='YlOrRd', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True)
ax.set_title("Distance to the Nearest Hospital for Each ZIP Code in Texas (Miles)")
ax.set_axis_off()
plt.show()
```


## Effects of closures on access in Texas (15 pts)

1. 
2. 
3. 
4. 

## Reflecting on the exercise (10 pts) 


2.the average distance to nearest hospital is 3.77 miles which means so many hospitals closed and therefore access to healthcare and urgent care has worsened. git
